{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc8htyitDLz0"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/research/PROJ201 20221/Detect English Text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "y_hZCctWE_4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('all_annotated.tsv', delimiter='\\t')"
      ],
      "metadata": {
        "id": "KqlY5jrWFIZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_en = data.loc[data['Definitely English']==1]\n",
        "data_en"
      ],
      "metadata": {
        "id": "2j4xOBjrFSlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_non_en = data.loc[data['Definitely Not English']==1]\n",
        "data_non_en"
      ],
      "metadata": {
        "id": "jeIfkjJ8FreI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([data_en, data_non_en]).reset_index(drop=True)\n",
        "data"
      ],
      "metadata": {
        "id": "Np3lpjm6FwCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['Tweet', 'Definitely English', 'Definitely Not English']]\n",
        "data"
      ],
      "metadata": {
        "id": "NVAgv_5cF8Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_label(definitely_en, definitely_non_en):\n",
        "  if definitely_en==1:\n",
        "    return 'en'\n",
        "  else:\n",
        "    return 'non-en'"
      ],
      "metadata": {
        "id": "fVUT5nK4GH9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['label'] = data.apply(lambda row : define_label(row['Definitely English'], row['Definitely Not English']), axis=1)\n",
        "data"
      ],
      "metadata": {
        "id": "LQeVmn1SGVMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del data['Definitely English'], data['Definitely Not English']\n",
        "data"
      ],
      "metadata": {
        "id": "NUzvvGQeG2uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_emojis(data):\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', data)"
      ],
      "metadata": {
        "id": "oMXnBke4Iu9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def remove_punctuations(text):\n",
        "  new_text = ''\n",
        "  for ch in text:\n",
        "    if not (ch in string.punctuation):\n",
        "      new_text += ch\n",
        "  return new_text"
      ],
      "metadata": {
        "id": "tScA5gTYJBkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweets(tweet):\n",
        "  # Remove Emoji\n",
        "  tweet = remove_emojis(tweet)\n",
        "  # Remove tags\n",
        "  tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", tweet)\n",
        "  # Remove hashtags\n",
        "  tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", tweet)\n",
        "  # Remove links\n",
        "  tweet = re.sub(r'http\\S+', '', tweet)\n",
        "  # Remove new line\n",
        "  tweet = re.sub('\\n+', ' ', tweet)\n",
        "  # tweet = re.sub('\\n', ' ', tweet)\n",
        "  # Remove emails\n",
        "  tweet = re.sub(r'[a-zA-Z\\d#!%\\$‘&\\+\\*–/=\\?\\^_`\\.\\{\\|\\}~]+@[a-zA-Z\\d]+\\.[a-zA-Z\\.]+', \"\", tweet)\n",
        "  # Remove punctuation\n",
        "  tweet = remove_punctuations(tweet)\n",
        "  # Remove Repeated spaces\n",
        "  tweet = re.sub(' +', ' ', tweet)\n",
        "  tweet = tweet.strip()\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "075EDAySITSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Tweet'] = data.apply(lambda row : clean_tweets(row['Tweet']), axis=1)\n",
        "data"
      ],
      "metadata": {
        "id": "HdJoNrkFJ1U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['delete'] = data.apply(lambda row : len(row['Tweet'])==0, axis=1)\n",
        "data"
      ],
      "metadata": {
        "id": "I50h5YMmKlsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.loc[data['delete']==False]\n",
        "del data['delete']\n",
        "data"
      ],
      "metadata": {
        "id": "o8lcne16KuLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of tweets:', len(data))\n",
        "print('Percentage of English Tweets:', (data['label']=='en').sum()/len(data))\n",
        "print('Percentage of Non-English Tweets:', (data['label']=='non-en').sum()/len(data))"
      ],
      "metadata": {
        "id": "OaPmq_P3K1S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLD54DPSLF76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "Jw7f7en5Okpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import DetectorFactory, detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "DetectorFactory.seed = 7\n",
        "\n",
        "def langdetect_predict(txt):\n",
        "    try:\n",
        "        if detect(txt) != \"en\":\n",
        "            return 'non-en'\n",
        "    except LangDetectException:\n",
        "        return 'non-en'\n",
        "    return 'en'"
      ],
      "metadata": {
        "id": "RrvwoaWpO88w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['langdetect_pred'] = data.apply(lambda row : langdetect_predict(row['Tweet']), axis=1)\n",
        "data"
      ],
      "metadata": {
        "id": "1GLLcIZhQGOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pycld2"
      ],
      "metadata": {
        "id": "jv2F13agQVGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import regex\n",
        "def remove_bad_chars(text):\n",
        "  RE_BAD_CHARS = regex.compile(r\"[\\p{Cc}\\p{Cs}]+\")\n",
        "  return RE_BAD_CHARS.sub(\"\", text)"
      ],
      "metadata": {
        "id": "1zJiyg-tRYVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pycld2 as cld2\n",
        "def pycld2_predict(txt):\n",
        "  txt = remove_bad_chars(txt)\n",
        "  isReliable, textBytesFound, details, vectors = cld2.detect(txt, returnVectors=True)\n",
        "  total_score = 0\n",
        "  enslish_score = 0\n",
        "  for v in vectors:\n",
        "    l = v[2].lower()\n",
        "    if l == 'english':\n",
        "      enslish_score += v[1]\n",
        "    total_score += v[1]\n",
        "\n",
        "  score = enslish_score/total_score if total_score>0 else 0\n",
        "  pred_lang = 'en' if score > 0.8 else 'non-en'\n",
        "  # score = score if pred_lang=='en' else (1-score)\n",
        "  return pred_lang"
      ],
      "metadata": {
        "id": "XJljIrJOQvQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['pycld2_pred'] = data.apply(lambda row : pycld2_predict(row['Tweet']), axis=1)\n",
        "data"
      ],
      "metadata": {
        "id": "nF-g7STbRtgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langid"
      ],
      "metadata": {
        "id": "0nzrMXF0RwTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langid\n",
        "def langid_predict(txt):\n",
        "    pred = langid.classify(txt)[0]\n",
        "    if pred=='en':\n",
        "      return 'en'\n",
        "    else:\n",
        "      return 'non-en'"
      ],
      "metadata": {
        "id": "UmXHCkv1SSM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['langid_pred'] = data.apply(lambda row : langid_predict(row['Tweet']), axis=1)"
      ],
      "metadata": {
        "id": "UzQCay06See4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "spQloyqbSvIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "nACK2btUSyOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(data['label'], data['pycld2_pred'],labels=[\"en\", \"non-en\"])\n",
        "print(cm)\n",
        "print(classification_report(data['label'], data['pycld2_pred'],labels=[\"en\", \"non-en\"]))"
      ],
      "metadata": {
        "id": "Uj3-hCkoTSq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(data['label'], data['langid_pred'],labels=[\"en\", \"non-en\"])\n",
        "print(cm)\n",
        "print(classification_report(data['label'], data['langid_pred'],labels=[\"en\", \"non-en\"]))"
      ],
      "metadata": {
        "id": "GKWZhRhoUH_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the conusion matix, precision, recall and accuray for langdetect prediction"
      ],
      "metadata": {
        "id": "Qt9ggdixrX0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}